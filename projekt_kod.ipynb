{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projekt kod",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesyraf/-Text-mining-based-on-tweets-of-US-presidential-candidates/blob/main/projekt_kod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghdpTsMFOr_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da55d64-96d4-4793-cf43-1968465e3a96"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import nltk as nltk\n",
        "%pip install nltk\n",
        "%pip install re\n",
        "%pip install gensin\n",
        "%pip install seaborn\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for re\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement gensin (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for gensin\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaP8EvOgQgK0",
        "outputId": "b553c486-ea1d-4133-a4b9-192a64fc3563"
      },
      "source": [
        "%pip install pickle5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 20.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 6.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219264 sha256=38e291274eadad221134da453ea73ee7f4f3c20a9bfe920989edf7b28c3f069c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9OzmUQZYxJS"
      },
      "source": [
        "import pickle5 as pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYQdbjWvGKzq"
      },
      "source": [
        "tweets_text = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvDwPFtHXIQQ"
      },
      "source": [
        "filename = \"Election_tweets.pkl\"\n",
        "\n",
        "with open(filename, \"rb\") as file:\n",
        "  while True:\n",
        "    try:\n",
        "      tweet = pickle.load(file)\n",
        "      tweets_text.append(tweet['full_text'])\n",
        "    except EOFError:\n",
        "      break\n",
        "\n",
        "df = pd.DataFrame(text, columns=[\"Tweets\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1bai9wTJoUf",
        "outputId": "5440bf7c-7e9f-4504-a0f8-e86c8fabe3f3"
      },
      "source": [
        "\"MrTrumpSpeeches.csv\".encode('utf-8').strip()\n",
        "\"joe_biden_dnc_2020.csv\".encode('utf-8').strip()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'joe_biden_dnc_2020.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dml8lAMlOzBI"
      },
      "source": [
        "Trump = pd.read_csv(\"MrTrumpSpeeches.csv\", encoding= 'latin-1', sep=\"~\")\n",
        "Biden = pd.read_csv(\"joe_biden_dnc_2020.csv\", encoding= 'latin-1', sep=\"~\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiqSW7u7O0bV"
      },
      "source": [
        "Trump_string = \" \".join(Trump['subtitles'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JiTRpAIPS21"
      },
      "source": [
        "Biden_string = \" \".join(Biden['TEXT'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA82AhnEPeVu"
      },
      "source": [
        "PREPROCESING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFGTCV57Pf_B"
      },
      "source": [
        "#usuwanie stopwordsów\n",
        "#usuwanie znaków specjalnych (\"emotikony\", interpunkcyjne)\n",
        "#usuwanie liczb\n",
        "#usuwanie duplikatów\n",
        "#tokenizacja\n",
        "#stemantyzacja, lemantyzacja\n",
        "#znajdywanie bigramów i trigramów"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXQB-L-Jfoj0"
      },
      "source": [
        "Podstawowe statystyki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jG1X8QXZYuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ee689c-4b02-4323-e5f7-2cbcb2f2735d"
      },
      "source": [
        "#lista stopwordsów\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siuuRRGqZ6P8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d47fff-4e1c-4d67-f426-04cef9724fc4"
      },
      "source": [
        "len(Trump_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15561807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etapwJcraoQU"
      },
      "source": [
        "#tokenizacja\n",
        "Trump_string.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNI7p58MbjYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a637ada4-de38-4beb-df4e-aa65a6509110"
      },
      "source": [
        "Trump_string_split = Trump_string.split()\n",
        "len([word for word in Trump_string_split if word in stop_words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1350439"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLdPXogKdGF3"
      },
      "source": [
        "#liczba stopwordsów\n",
        "Trump['liczba_stopwords'] = Trump['subtitles'].apply(lambda x: len ([word for word in x.split() if word in stop_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0bsPQMqxvIR"
      },
      "source": [
        "Trump['zmalych'] = Trump['subtitles'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJae3eA1x9a3"
      },
      "source": [
        "Trump['stopwords'] = Trump['zmalych'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcJfjduov8oF"
      },
      "source": [
        "Trump['tokeny'] = Trump['stopwords'].apply(lambda x: x.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rvsE1-7ik0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f10da2f-021f-43b3-a360-0411cc8d751d"
      },
      "source": [
        "#sprawdzanie ile występuje liczb\n",
        "len([word for word in Trump_string if word.isdigit])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15561807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KttmQg-viiCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da2d79f-6e8e-4422-aa1c-7dc07fc23a45"
      },
      "source": [
        "#liczba słów w tekście\n",
        "len(Trump_string.split(\" \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3574984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2SFJbEli9nv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607120b6-5b60-4591-d535-74797f47ba28"
      },
      "source": [
        "#liczba znaków, która zwiera spacje\n",
        "len(Trump_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15561807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqyRjNY2cKza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53dd8c0-7f6f-443b-a211-f842771fa049"
      },
      "source": [
        "#liczba słów napisanych z dużych liter\n",
        "len([word for word in Trump_string.split(\" \") if word.isupper])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3574984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A0a3UDNj33K"
      },
      "source": [
        "słowa_Trumpa_z_dużych_liter = ([word for word in Trump_string.split(\" \") if word.isupper])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXaVhEBokDyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "d4499e8e-f9d8-4f53-80bd-4e124c7c8101"
      },
      "source": [
        "#pokazać liczbę \"I\"\n",
        "len(for word in słowa_Trumpa_z_dużych_liter):\n",
        "  if \"I\" in słowa_Trumpa_z_dużych_liter:\n",
        "    print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-27c02b0daa44>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    len(for word in słowa_Trumpa_z_dużych_liter):\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejuG_P-wnn5h"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1DDr4WUnq-Y"
      },
      "source": [
        "#zmiana na małe litery wszystkiego\n",
        "data = Trump_string.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBYyDj8od1h"
      },
      "source": [
        "#usuwanie interpunkcji\n",
        "#strona regexr.com może pomóc w znalezieniu rzeczy które chcemy usunąc z tekstu\n",
        "\n",
        "#znajduje konkretne wyrażenie i zamienia je na \"nic\" (usuwa interpunkcję)\n",
        "data = re.sub('[^\\w\\s], \"\", data')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PEIuI0rp1ml"
      },
      "source": [
        "#liczba liczb w data\n",
        "len([word for word in data if word.isdigit{}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GirYXzwSqNRd"
      },
      "source": [
        "#usuwanie liczb\n",
        "data = re.sub('\\d+',\"\", data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx4MqRmbrJRS"
      },
      "source": [
        "#tokenizacja przed usuwaniem stopwordsów\n",
        "tokenized_words = data.split() #dzieli na słowa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUFT7oCJreKv"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tkoenized_word_1 = word_tokenized(data) #to samo co wyżej tylko inny sposób"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qp9mF4-r46S"
      },
      "source": [
        "#usuwanie stopwordsów\n",
        "filtered_word = []\n",
        "for word in tokenized_words:\n",
        "    if word not in stop_words:\n",
        "        filtered_word.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_KYy3Q2slJ0"
      },
      "source": [
        "filtered_word #słowa bez stopwordsów"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgokf438tjli"
      },
      "source": [
        "Częstość występowania słów"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gEYz1KdtslY"
      },
      "source": [
        "fdist = FreqDist(filtered_word)\n",
        "common = fdist.most_common(20) #top20 najczęściej występujących"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQs6k0q4Wnlk"
      },
      "source": [
        "tablica=pd.DataFrame(common, columns=['words','count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6i-y961XpgM"
      },
      "source": [
        "#robimy wykresy dzięki bibliotece seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Q_oINFYW7B"
      },
      "source": [
        "fig, as = plt.subplots(figsize=(10,10))\n",
        "colors = sns.color_palette (\"husl\",10)\n",
        "tablica.sort_values(by='count').plot.barh(x='words',y='count', ax=ax, color = colors, alpha=0,5)\n",
        "\n",
        "ax.set.title('najczęściejwystępujące słowa w wypowiedziach Trumpa')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfuqIs0jZ1mb"
      },
      "source": [
        "Chmury słów\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4HCQ1q_Z5IT"
      },
      "source": [
        " pip install wordcloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVqVkJQWZ8uC"
      },
      "source": [
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "from matplotlib import cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v_2quJVaIyI"
      },
      "source": [
        "color = cm.Greens\n",
        "\n",
        "wc = WordCloud(background_color ='white', colormap = color, \n",
        "               max_words = 500, #max_font_size = 20, width=2000, height=1500)\n",
        "\n",
        "wc.generate(data)\n",
        "\n",
        "plt.figure(figsize=[10,10])\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off');  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lQwzlv8cBUz"
      },
      "source": [
        "pip install PIL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxMK4nFvcDbW"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LvQSGtedMto"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr9MyvSAcL6q"
      },
      "source": [
        "mask = np.array(Image.open('jakieśzdjęcie.png'))\n",
        "imagine_colors = ImagineColorGenerator(mask)\n",
        "\n",
        "wc = WordCloud(background_color ='white', mask = mask, max_words = 50000, contour_width = 0.01)\n",
        "my_wordcloud = wc.generate(data)\n",
        "\n",
        "plt.figure(figsize=[10,10])\n",
        "plt.imshow(my_wordcloud.recolor(color_func = image_colors), interpolation='bilinear')\n",
        "plt.axis('off');\n",
        "plt.savefig(\"wordcloud.png\", format='png') #zapisuje to co zrobiliśmy  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpUhGLSedwaL"
      },
      "source": [
        "Bigramy i trigramy (podwójne i potrójne słowa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wQQoTEsdywr"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX2ExrRNd2wn"
      },
      "source": [
        "pip install collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LoHf3Y1d7W9"
      },
      "source": [
        "import collections\n",
        "from nltk import bigrams, trigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzf8K4JFeHJr"
      },
      "source": [
        "bigramy = list(bigrams(filtered_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP8rQFLmepwC"
      },
      "source": [
        "trigramy = list(trigrams(filtered_word))\n",
        "frequency_trigram = collections.Counter(trigramy)\n",
        "common_trigram = frequency.most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC6fUMWze7-p"
      },
      "source": [
        "Stematyzacja i lematyzacja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2mW4pn_e_KY"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CViUGc4fThI"
      },
      "source": [
        "ps = PortStemmer ()\n",
        "\n",
        "stemmed_words = []\n",
        "\n",
        "for word in filtered_word:\n",
        "  stemmed_words.append(ps.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPwYhGl1gVYS"
      },
      "source": [
        "lem = WordNetLemmatizer ()\n",
        "\n",
        "lemmed_words = [] #można to też wrzucać normalnie do bigramów i trigramów\n",
        "\n",
        "for word in filtered_word:\n",
        "  lemmed_words.append(lem.lemmatize(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmZHeEM1fct6"
      },
      "source": [
        "filtered_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of0s8e2yg2Vv"
      },
      "source": [
        "Preprocessing DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzYZ7L5Rg7kC"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAZioh4RhBK9"
      },
      "source": [
        "Trump = pd.read_csv(\"MrTrumpSpeeches.csv\", sep=\"~\", encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_IWqHGWPXUk"
      },
      "source": [
        "Biden=pd.read_csv(\"joe_biden_dnc_2020.csv\", sep=\"~\", encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Wb_xHahQhJ"
      },
      "source": [
        "Trump['filtered_words'] = Trump['subtitles'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH_kMvf5PyKr"
      },
      "source": [
        "Biden['filtered_words1']= Biden['TEXT'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3_Zx_TehgaJ"
      },
      "source": [
        "Trump['filtered_words'] = Trump['filtered_words'].apply(lambda x: re.sub('\\d+', \"\", x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1zQMVyyQUBf"
      },
      "source": [
        "Biden['filtered_words1']=Biden['filtered_words1'].apply(lambda x: re.sub('\\d+', \"\",x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwJGMfhTh521"
      },
      "source": [
        "Trump['filtered_words'] = Trump['filtered_words'].apply(lambda x: re.sub('[^\\\\w\\s]', \"\", x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anmP4PkuRRhV"
      },
      "source": [
        "Biden['filtered_words1']= Biden['filtered_words1'].apply (lambda x:re.sub ('[^\\\\w\\s]',\"\",x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlZ7wKfWiHt0"
      },
      "source": [
        "def remove_my_stopwords(text):\n",
        "  stop_words = stopwords.words(\"english\")\n",
        "  stop_words.extend(['la', '[music]'])\n",
        "  filtered_word = []\n",
        "  for word in text.split():\n",
        "    if word not in stop_words:\n",
        "      filtered_word.append(word)\n",
        "  return filtered_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRJ1nTvMR_fq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgBw_a4Fivqo"
      },
      "source": [
        "Trump['filtered_words'] = Trump['filtered_words'].apply(lambda x: remove_my_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dwKjLE_jG0R"
      },
      "source": [
        "Term frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd8nmwEGjI4z"
      },
      "source": [
        "dane = []\n",
        "for i in range(len(Trump)):\n",
        "    for j in Trump['filtered_words'][i]:\n",
        "        dane.append(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyKX2fQgSUxL"
      },
      "source": [
        "dane1=[]\n",
        "for i in range (len(Biden)):\n",
        "  for j in Biden ['filtered_words1'][i]:\n",
        "    dane1.append (j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfy7BTm2jW8j"
      },
      "source": [
        "df1 = pd.DataFrame(dane, columns=['words'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut18PnqySmsY"
      },
      "source": [
        "df2=pd.DataFrame(dane1, columns=['words'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NqmizcAjfaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a359528f-541b-4584-d426-0752148f7b09"
      },
      "source": [
        "round(df1.words.value_counts()/len(df1),4)*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "going          1.97\n",
              "people         1.38\n",
              "know           1.21\n",
              "dont           0.80\n",
              "great          0.78\n",
              "               ... \n",
              "antagonist     0.00\n",
              "fairgrounds    0.00\n",
              "stroking       0.00\n",
              "patchy         0.00\n",
              "rauf           0.00\n",
              "Name: words, Length: 23067, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th5ceq1XSwnD"
      },
      "source": [
        "round(df2.words.value_counts()/len(df2),4)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c4osa2Cjrp8"
      },
      "source": [
        "Document Term Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzGPOBujutP"
      },
      "source": [
        "most_frequent1000 = df1.words.value_counts().head(1000).keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq56DqItTTa0"
      },
      "source": [
        "most_frequent1000=df2.words.value_counts().head(1000).keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TcMSAppj5Le"
      },
      "source": [
        "freq = pd.DataFrame(index=most_frequent1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXgRGJJ1TezL"
      },
      "source": [
        "freq= pd.DataFrame(index=most_frequent1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTPEJTVEjjTj"
      },
      "source": [
        "for index,row in Trump.iterrows(): #pętla po każdym wierzu w ramce danych Trump\n",
        "    frequences = [] #utworzenie pustej listy\n",
        "    for word in most_frequent1000: #pętla po każdym słowie z listy najczęściej występujących słów\n",
        "        frequences.append(row.filtered_words.count(word)) \n",
        "        #obliczanie częśtości wystąpienia słowa w wypowiedzi i zapis do listy\n",
        "    freq[f\"(row.title)\"] = frequences #zapis częstoci do ramki danych"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pv--Ia_Tm7I"
      },
      "source": [
        "for index,row in Biden.iterrows(): #pętla po każdym wierszu w ramce danych Trump\n",
        "    frequences = [] #utworzenie pustej listy\n",
        "    for word in most_frequent1000: #pętla po każdym słowie z listy najczęściej występujących słów\n",
        "        frequences.append(row.filtered_words1.count(word)) \n",
        "        #obliczanie częśtości wystąpienia słowa w wypowiedzi i zapis do listy\n",
        "    freq[f\"(row.title)\"] = frequences #zapis częstoci do ramki danych"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bF4jQrIUZTG"
      },
      "source": [
        "freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA73VAQPkhnn"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(20,20))\n",
        "\n",
        "ax.set_title(\"Częstość występowania słów w wypowiedziach Trumpa\", fontsize=20)\n",
        "wykres = sns.heatmap(freq, vmin=0, vmax=50, cmap=\"GnBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgVSIZcwk_Mp"
      },
      "source": [
        "Grupowanie hierarchiczne, aglomeracyjne"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP39HQxolB3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad75cba1-a534-4576-8bdb-b281a27a183a"
      },
      "source": [
        "pip install scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVxYAeizlEg-"
      },
      "source": [
        "from scipy.cluster.hierarchy import cophenet, dendrogram, linkage, fcluster\n",
        "from scipy.spatial.distance import pdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aHzqmTEWfKq"
      },
      "source": [
        "freq = (freq.T).head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHWsBTHslP5A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "91fe120b-c5c1-41a0-ae36-8ce7306f9dcb"
      },
      "source": [
        "Y = pdist(freq,'cosine');\n",
        "linked=linkage(Y,'complete');\n",
        "plt.figure(figsize=(20, 10))\n",
        "labels=freq.index\n",
        "labels=labels.to_list()\n",
        "dendrogram(linked, labels=labels)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-94b90bcaced1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cosine'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/cluster/hierarchy.py\u001b[0m in \u001b[0;36mlinkage\u001b[0;34m(y, method, metric, optimal_ordering)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                          \"finite values.\")\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_obs_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m     \u001b[0mmethod_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LINKAGE_METHODS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mnum_obs_y\u001b[0;34m(Y)\u001b[0m\n\u001b[1;32m   2401\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2403\u001b[0;31m         raise ValueError(\"The number of observations cannot be determined on \"\n\u001b[0m\u001b[1;32m   2404\u001b[0m                          \"an empty distance matrix.\")\n\u001b[1;32m   2405\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The number of observations cannot be determined on an empty distance matrix."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnTos38k0iKX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}